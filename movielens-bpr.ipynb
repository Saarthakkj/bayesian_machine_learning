{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd5b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ml-32m-20241011T132018Z-001/ml-32m/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31b81ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId\n",
       "0       1       17\n",
       "1       1       25\n",
       "2       1       29\n",
       "3       1       30\n",
       "4       1       32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop( columns = ['rating' , 'timestamp'] , inplace = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d970dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.200020e+07</td>\n",
       "      <td>3.200020e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.002785e+05</td>\n",
       "      <td>2.931861e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.794905e+04</td>\n",
       "      <td>5.095816e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.005300e+04</td>\n",
       "      <td>1.233000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.002970e+05</td>\n",
       "      <td>3.452000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.504510e+05</td>\n",
       "      <td>4.419900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.009480e+05</td>\n",
       "      <td>2.927570e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId       movieId\n",
       "count  3.200020e+07  3.200020e+07\n",
       "mean   1.002785e+05  2.931861e+04\n",
       "std    5.794905e+04  5.095816e+04\n",
       "min    1.000000e+00  1.000000e+00\n",
       "25%    5.005300e+04  1.233000e+03\n",
       "50%    1.002970e+05  3.452000e+03\n",
       "75%    1.504510e+05  4.419900e+04\n",
       "max    2.009480e+05  2.927570e+05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0c619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32000204 entries, 0 to 32000203\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   userId   int64\n",
      " 1   movieId  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 488.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae208745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12800081 entries, 10128624 to 11717299\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   userId   int64\n",
      " 1   movieId  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 293.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3200021 entries, 20566633 to 19454882\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   userId   int64\n",
      " 1   movieId  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 73.2 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is already defined\n",
    "# Shorten the length of the data to 50%\n",
    "df = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Now splitting the data into training and testing split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the splits to CSV files\n",
    "train_df.to_csv('train_data.txt', index=False)\n",
    "test_df.to_csv('test_data.txt', index=False)\n",
    "\n",
    "# Display information about the splits\n",
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8dcec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12800081 entries, 10128624 to 11717299\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   userId   int64\n",
      " 1   movieId  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 293.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66e1c08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T03:39:32.552114Z",
     "iopub.status.busy": "2024-10-07T03:39:32.551526Z",
     "iopub.status.idle": "2024-10-07T03:39:32.576869Z",
     "shell.execute_reply": "2024-10-07T03:39:32.575757Z",
     "shell.execute_reply.started": "2024-10-07T03:39:32.552075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def topK_scores(test, predict, topk, user_count, item_count):\n",
    "    PrecisionSum = np.zeros(topk+1)\n",
    "    RecallSum = np.zeros(topk+1)\n",
    "    F1Sum = np.zeros(topk+1)\n",
    "    NDCGSum = np.zeros(topk+1)\n",
    "    OneCallSum = np.zeros(topk+1)\n",
    "    MRRSum = 0\n",
    "    MAPSum = 0\n",
    "    total_test_data_count = 0\n",
    "    \n",
    "    # Precompute DCGbest for efficiency\n",
    "    DCGbest = np.zeros(topk+1)\n",
    "    for k in range(1, topk+1):\n",
    "        DCGbest[k] = DCGbest[k - 1] + 1.0 / math.log2(k + 1)\n",
    "    \n",
    "    # Loop over each user\n",
    "    for i in range(user_count):\n",
    "        user_test = test[i * item_count:(i + 1) * item_count]\n",
    "        user_predict = predict[i * item_count:(i + 1) * item_count]\n",
    "\n",
    "        # Get test data size for this user\n",
    "        test_data_size = np.sum(user_test)\n",
    "        if test_data_size == 0:\n",
    "            continue\n",
    "        total_test_data_count += 1\n",
    "\n",
    "        # Get top-k item indices for this user\n",
    "        top_k_indices = np.argsort(user_predict)[-topk:][::-1]  # Sort in descending order\n",
    "        \n",
    "        hit_sum = 0\n",
    "        DCG = np.zeros(topk + 1)\n",
    "        for k in range(1, topk + 1):\n",
    "            item_id = top_k_indices[k - 1]\n",
    "            if user_test[item_id] == 1:\n",
    "                hit_sum += 1\n",
    "                DCG[k] = DCG[k - 1] + 1 / math.log2(k + 1)\n",
    "            else:\n",
    "                DCG[k] = DCG[k - 1]\n",
    "            \n",
    "            prec = hit_sum / k\n",
    "            rec = hit_sum / test_data_size\n",
    "            f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "            PrecisionSum[k] += prec\n",
    "            RecallSum[k] += rec\n",
    "            F1Sum[k] += f1\n",
    "            NDCGSum[k] += DCG[k] / DCGbest[k]\n",
    "            OneCallSum[k] += 1 if hit_sum > 0 else 0\n",
    "\n",
    "        # Compute MRR\n",
    "        for rank, idx in enumerate(top_k_indices, start=1):\n",
    "            if user_test[idx] == 1:\n",
    "                MRRSum += 1 / rank\n",
    "                break\n",
    "\n",
    "        # Compute MAP\n",
    "        AP = 0\n",
    "        hit_count = 0\n",
    "        for rank, idx in enumerate(top_k_indices, start=1):\n",
    "            if user_test[idx] == 1:\n",
    "                hit_count += 1\n",
    "                AP += hit_count / rank\n",
    "        MAPSum += AP / test_data_size\n",
    "    \n",
    "    # Normalize metrics\n",
    "    total_test_data_count = max(1, total_test_data_count)  # Avoid division by 0\n",
    "    print('MAP:', MAPSum / total_test_data_count)\n",
    "    print('MRR:', MRRSum / total_test_data_count)\n",
    "    print('Prec@5:', PrecisionSum[5] / total_test_data_count)\n",
    "    print('Rec@5:', RecallSum[5] / total_test_data_count)\n",
    "    print('F1@5:', F1Sum[5] / total_test_data_count)\n",
    "    print('NDCG@5:', NDCGSum[5] / total_test_data_count)\n",
    "    print('1-call@5:', OneCallSum[5] / total_test_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff705f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train and test data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Convert the dataframes to lists of tuples\n",
    "train_user_item_pairs = list(train_df.itertuples(index=False, name=None))  # [(userid, movieid), ...]\n",
    "test_user_item_pairs = list(test_df.itertuples(index=False, name=None))  # [(userid, movieid), ...]\n",
    "\n",
    "# Define the filtering function\n",
    "def filter_data(user_item_pairs, min_interactions=10):\n",
    "    # Step 1: Count interactions\n",
    "    user_counts = defaultdict(set)\n",
    "    item_counts = defaultdict(set)\n",
    "    \n",
    "    for user, item in user_item_pairs:\n",
    "        user_counts[user].add(item)\n",
    "        item_counts[item].add(user)\n",
    "\n",
    "    # Step 2: Filter out users with less than `min_interactions` items\n",
    "    filtered_users = {user: items for user, items in user_counts.items() if len(items) >= min_interactions}\n",
    "    \n",
    "    # Step 3: Filter out items with less than `min_interactions` users\n",
    "    filtered_items = {item: users for item, users in item_counts.items() if len(users) >= min_interactions}\n",
    "\n",
    "    # Step 4: Remove any remaining users/items that no longer meet the conditions\n",
    "    while True:\n",
    "        # Remove items from users that don't exist in filtered items\n",
    "        new_filtered_users = {user: {item for item in items if item in filtered_items} for user, items in filtered_users.items()}\n",
    "        # Remove users that now have fewer than `min_interactions` items\n",
    "        new_filtered_users = {user: items for user, items in new_filtered_users.items() if len(items) >= min_interactions}\n",
    "        \n",
    "        # Remove users from items that don't exist in filtered users\n",
    "        new_filtered_items = {item: {user for user in users if user in new_filtered_users} for item, users in filtered_items.items()}\n",
    "        # Remove items that now have fewer than `min_interactions` users\n",
    "        new_filtered_items = {item: users for item, users in new_filtered_items.items() if len(users) >= min_interactions}\n",
    "\n",
    "        # Check if the filtering stabilized\n",
    "        if new_filtered_users == filtered_users and new_filtered_items == filtered_items:\n",
    "            break\n",
    "        \n",
    "        filtered_users, filtered_items = new_filtered_users, new_filtered_items\n",
    "\n",
    "    # Convert filtered data back to a list of duplets\n",
    "    filtered_user_item_pairs = [(user, item) for user, items in filtered_users.items() for item in items]\n",
    "    \n",
    "    return filtered_user_item_pairs\n",
    "\n",
    "# Apply filtering to train and test data\n",
    "filtered_train_pairs = filter_data(train_user_item_pairs)\n",
    "filtered_test_pairs = filter_data(test_user_item_pairs)\n",
    "\n",
    "# Convert the filtered pairs back to DataFrames\n",
    "filtered_train_df = pd.DataFrame(filtered_train_pairs, columns=['userid', 'movieid'])\n",
    "filtered_test_df = pd.DataFrame(filtered_test_pairs, columns=['userid', 'movieid'])\n",
    "\n",
    "# Overwrite the filtered DataFrames to their respective text files\n",
    "filtered_train_df.to_csv(\"train_data_filtered.txt\", index=False, header=False)\n",
    "filtered_test_df.to_csv(\"test_data_filtered.txt\", index=False, header=False)\n",
    "\n",
    "print(\"Filtered train and test data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d890e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:17:00.438134Z",
     "iopub.status.busy": "2024-10-07T09:17:00.437797Z",
     "iopub.status.idle": "2024-10-07T09:17:00.623880Z",
     "shell.execute_reply": "2024-10-07T09:17:00.622778Z",
     "shell.execute_reply.started": "2024-10-07T09:17:00.438099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining users: 181686\n",
      "Remaining items: 21570\n"
     ]
    }
   ],
   "source": [
    "remaining_users = filtered_train_df['userid'].nunique()  # Count unique users\n",
    "remaining_items = filtered_train_df['movieid'].nunique()  # Count unique items\n",
    "\n",
    "print(f\"Remaining users: {remaining_users}\")\n",
    "print(f\"Remaining items: {remaining_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scores\n",
    "\n",
    "class BPR:\n",
    "    user_count = 943\n",
    "    item_count = 1682\n",
    "    latent_factors = 20\n",
    "    lr = 0.01\n",
    "    reg = 0.01\n",
    "    train_count = 1000\n",
    "    train_data_path = 'train.txt'\n",
    "    test_data_path = 'test.txt'\n",
    "    size_u_i = user_count * item_count\n",
    "    # latent_factors of U & V\n",
    "    U = np.random.rand(user_count, latent_factors) * 0.01\n",
    "    V = np.random.rand(item_count, latent_factors) * 0.01\n",
    "    biasV = np.random.rand(item_count) * 0.01\n",
    "    test_data = np.zeros((user_count, item_count))\n",
    "    test = np.zeros(size_u_i)\n",
    "    predict_ = np.zeros(size_u_i)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        user_ratings = defaultdict(set)\n",
    "        max_u_id = -1\n",
    "        max_i_id = -1\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                u, i = line.split(\" \")\n",
    "                u = int(u)\n",
    "                i = int(i)\n",
    "                user_ratings[u].add(i)  \n",
    "                max_u_id = max(u, max_u_id)\n",
    "                max_i_id = max(i, max_i_id)\n",
    "        return user_ratings\n",
    "\n",
    "    def load_test_data(self, path):\n",
    "        file = open(path, 'r')\n",
    "        for line in file:\n",
    "            line = line.split(' ')\n",
    "            user = int(line[0])\n",
    "            item = int(line[1])\n",
    "            self.test_data[user - 1][item - 1] = 1\n",
    "\n",
    "    def train(self, user_ratings_train):\n",
    "        for user in range(self.user_count):\n",
    "            # sample a user\n",
    "            u = random.randint(1, self.user_count)\n",
    "            if u not in user_ratings_train.keys():\n",
    "                continue\n",
    "            # sample a positive item from the observed items\n",
    "            i = random.sample(user_ratings_train[u], 1)[0] #sample(user,1) 的作用是从user_rating_train[u]中随机选取一个元素\n",
    "            # sample a negative item from the unobserved items\n",
    "            j = random.randint(1, self.item_count)\n",
    "            while j in user_ratings_train[u]:\n",
    "                j = random.randint(1, self.item_count)\n",
    "            u -= 1\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            r_ui = np.dot(self.U[u], self.V[i].T) + self.biasV[i]\n",
    "            r_uj = np.dot(self.U[u], self.V[j].T) + self.biasV[j]\n",
    "            r_uij = r_ui - r_uj\n",
    "            loss_func = -1.0 / (1 + np.exp(r_uij))\n",
    "            # update U and V\n",
    "            self.U[u] += -self.lr * (loss_func * (self.V[i] - self.V[j]) + self.reg * self.U[u])\n",
    "            self.V[i] += -self.lr * (loss_func * self.U[u] + self.reg * self.V[i])\n",
    "            self.V[j] += -self.lr * (loss_func * (-self.U[u]) + self.reg * self.V[j])\n",
    "            # update biasV\n",
    "            self.biasV[i] += -self.lr * (loss_func + self.reg * self.biasV[i])\n",
    "            self.biasV[j] += -self.lr * (-loss_func + self.reg * self.biasV[j])\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        predict = np.mat(user) * np.mat(item.T)\n",
    "        return predict\n",
    "\n",
    "    def main(self):\n",
    "        user_ratings_train = self.load_data(self.train_data_path)\n",
    "        self.load_test_data(self.test_data_path)\n",
    "        for u in range(self.user_count):\n",
    "            for item in range(self.item_count):\n",
    "                if int(self.test_data[u][item]) == 1:\n",
    "                    self.test[u * self.item_count + item] = 1\n",
    "                else:\n",
    "                    self.test[u * self.item_count + item] = 0\n",
    "        # training\n",
    "        for i in range(self.train_count):\n",
    "            self.train(user_ratings_train)\n",
    "        predict_matrix = self.predict(self.U, self.V)\n",
    "        # prediction\n",
    "        # getA()使得矩阵转换为数组narry，这样才可以取出其中的元素，负责会造成指针越界，而reshape(-1)则是把数组变成一行\n",
    "        self.predict_ = predict_matrix.getA().reshape(-1)\n",
    "        self.predict_ = pre_handel(user_ratings_train, self.predict_, self.item_count)\n",
    "        auc_score = roc_auc_score(self.test, self.predict_)\n",
    "        print('AUC:', auc_score)# auc=(area under curve)\n",
    "        # Top-K evaluation\n",
    "        str(scores.topK_scores(self.test, self.predict_, 5, self.user_count, self.item_count))\n",
    "\n",
    "def pre_handel(set, predict, item_count):\n",
    "    # Ensure the recommendation cannot be positive items in the training set.\n",
    "    for u in set.keys():\n",
    "        for j in set[u]:\n",
    "            predict[(u - 1) * item_count + j - 1] = 0\n",
    "    return predict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bpr = BPR()\n",
    "    bpr.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c08dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:17:01.467985Z",
     "iopub.status.busy": "2024-10-07T09:17:01.467582Z",
     "iopub.status.idle": "2024-10-07T09:17:01.635250Z",
     "shell.execute_reply": "2024-10-07T09:17:01.633763Z",
     "shell.execute_reply.started": "2024-10-07T09:17:01.467945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253790/975528873.py:76: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  i = random.sample(user_ratings_train[u_original], 1)[0]  # Convert to list for sampling\n",
      "/tmp/ipykernel_253790/975528873.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  loss_func = -1.0 / (1 + np.exp(r_uij))  # Logistic function for BPR loss\n",
      "/tmp/ipykernel_253790/975528873.py:90: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  r_uij = r_ui - r_uj\n",
      "/tmp/ipykernel_253790/975528873.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  r_uij = r_ui - r_uj\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np  # Import NumPy for CPU support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class BPR:\n",
    "    def __init__(self, filtered_user_count, unique_item_count, latent_factors, lr, reg, filtered_train_df, filtered_test_df):\n",
    "        self.user_count = filtered_user_count\n",
    "        self.item_count = unique_item_count  # Unique item count\n",
    "        self.latent_factors = latent_factors\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.filtered_train_df = filtered_train_df  # Using filtered train DataFrame\n",
    "        self.filtered_test_df = filtered_test_df  # Using filtered test DataFrame\n",
    "\n",
    "        # Initialize latent factors for users and items\n",
    "        self.U = np.random.rand(self.user_count, self.latent_factors).astype(np.float32) * 0.01\n",
    "        self.V = np.random.rand(self.item_count, self.latent_factors).astype(np.float32) * 0.01\n",
    "        self.biasV = np.random.rand(self.item_count).astype(np.float32) * 0.01\n",
    "        \n",
    "        # Initialize test data using a dictionary for sparse representation\n",
    "        self.test_data = defaultdict(dict)  # Maps user index to item index with values\n",
    "        self.predict_ = np.zeros(self.user_count * self.item_count, dtype=np.float32)\n",
    "\n",
    "        # Initialize user and item mappings\n",
    "        self.user_mapping = {}\n",
    "        self.item_mapping = {}\n",
    "        self.reverse_user_mapping = {}\n",
    "        self.reverse_item_mapping = {}\n",
    "\n",
    "        # Create user and item mappings from filtered DataFrames\n",
    "        self.create_mappings()\n",
    "\n",
    "    def create_mappings(self):\n",
    "        user_id_set = self.filtered_train_df['userid'].unique()  # Extract unique user IDs\n",
    "        item_id_set = self.filtered_train_df['movieid'].unique()  # Extract unique item IDs\n",
    "\n",
    "        # Create a mapping for user IDs\n",
    "        for index, user_id in enumerate(user_id_set):\n",
    "            self.user_mapping[user_id] = index\n",
    "            self.reverse_user_mapping[index] = user_id\n",
    "\n",
    "        # Create a mapping for item IDs\n",
    "        for index, item_id in enumerate(item_id_set):\n",
    "            self.item_mapping[item_id] = index\n",
    "            self.reverse_item_mapping[index] = item_id\n",
    "\n",
    "    def load_test_data(self):\n",
    "        for _, row in self.filtered_test_df.iterrows():\n",
    "            user = int(row['userid'])\n",
    "            item = int(row['movieid'])\n",
    "\n",
    "            if user in self.user_mapping:  # Only process if user is in mapping\n",
    "                # Get the mapped index\n",
    "                mapped_user_index = self.user_mapping[user]\n",
    "                \n",
    "                if item in self.item_mapping:  # Check if the item is mapped\n",
    "                    mapped_item_index = self.item_mapping[item]\n",
    "                    # Use dictionary for test data\n",
    "                    self.test_data[mapped_user_index][mapped_item_index] = 1.0  # Mark interaction\n",
    "\n",
    "    def train(self, user_ratings_train, bootstrap=True):\n",
    "            \n",
    "            for _ in range(self.user_count):  # Number of iterations\n",
    "                try: \n",
    "                    # Bootstrap sample a user\n",
    "                    u_original = random.choice(list(user_ratings_train.keys())) if bootstrap else random.randint(1, self.user_count)\n",
    "                    if u_original not in user_ratings_train:\n",
    "                        continue\n",
    "\n",
    "                    # Map to the new user index\n",
    "                    u = self.user_mapping[u_original]\n",
    "\n",
    "                    # Sample a positive item (i) from user's interactions\n",
    "                    i = random.sample(user_ratings_train[u_original], 1)[0]  # Convert to list for sampling\n",
    "\n",
    "                    # Sample a negative item (j) not interacted by user\n",
    "                    j = random.randint(1, self.item_count)\n",
    "                    while j in user_ratings_train[u_original]:\n",
    "                        j = random.randint(1, self.item_count)\n",
    "\n",
    "                    # Decrement to make zero-indexed\n",
    "                    i = self.item_mapping[i]  # Map to new item index\n",
    "                    j = self.item_mapping[j]  # Map to new item index\n",
    "\n",
    "                    # Compute predictions for positive (i) and negative (j) items\n",
    "                    r_ui = np.dot(self.U[u], self.V[i]) + self.biasV[i]\n",
    "                    r_uj = np.dot(self.U[u], self.V[j]) + self.biasV[j]\n",
    "                    r_uij = r_ui - r_uj\n",
    "\n",
    "                    # BPR optimization: Compute the gradient and update using log-sigmoid\n",
    "                    loss_func = -1.0 / (1 + np.exp(r_uij))  # Logistic function for BPR loss\n",
    "\n",
    "                    # Update latent factors for user and items\n",
    "                    self.U[u] += self.lr * (loss_func * (self.V[i] - self.V[j]) - self.reg * self.U[u])\n",
    "                    self.V[i] += self.lr * (loss_func * self.U[u] - self.reg * self.V[i])\n",
    "                    self.V[j] += self.lr * (-loss_func * self.U[u] - self.reg * self.V[j])\n",
    "\n",
    "                    # Update biases for items\n",
    "                    self.biasV[i] += self.lr * (loss_func - self.reg * self.biasV[i])\n",
    "                    self.biasV[j] += self.lr * (-loss_func - self.reg * self.biasV[j])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    def predict(self):\n",
    "        # Prediction matrix for user-item interactions\n",
    "        predict_matrix = np.dot(self.U, self.V.T) + self.biasV\n",
    "        return predict_matrix\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Evaluate using AUC score\n",
    "        predictions = self.predict()\n",
    "        # Flatten the test data using a comprehension since it's a dict\n",
    "        test_flat = np.array([self.test_data[u].get(i, 0) for u in range(self.user_count) for i in range(self.item_count)], dtype=np.float32)\n",
    "        auc_score = roc_auc_score(test_flat, predictions.flatten())  # Convert to 1D array\n",
    "        print('AUC:', auc_score)\n",
    "\n",
    "    def main(self):\n",
    "        user_ratings_train = defaultdict(set)\n",
    "        \n",
    "        # Prepare user ratings from filtered_train_df\n",
    "        for _, row in self.filtered_train_df.iterrows():\n",
    "            user = int(row['userid'])\n",
    "            item = int(row['movieid'])\n",
    "            user_ratings_train[user].add(item)\n",
    "\n",
    "        self.load_test_data()\n",
    "\n",
    "        # Train model\n",
    "        for _ in range(1000):  # Number of training steps\n",
    "            self.train(user_ratings_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        self.evaluate()\n",
    "\n",
    "def cross_validate_bpr(filtered_user_count, unique_item_count, latent_factors, lr, reg, filtered_train_df, filtered_test_df, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(filtered_train_df):\n",
    "        train_df = filtered_train_df.iloc[train_index]\n",
    "        test_df = filtered_train_df.iloc[test_index]\n",
    "\n",
    "        bpr = BPR(filtered_user_count, unique_item_count, latent_factors, lr, reg, train_df, test_df)\n",
    "        bpr.main()\n",
    "        auc_scores.append(bpr.evaluate())\n",
    "\n",
    "    print(f'Mean AUC: {np.mean(auc_scores)}')\n",
    "    print(f'Standard Deviation of AUC: {np.std(auc_scores)}')\n",
    "\n",
    "# Running the BPR model with cross-validation\n",
    "if __name__ == '__main__':\n",
    "    filtered_user_count = 181686  # Updated number of unique users after filtering\n",
    "    unique_item_count = 21570  # Updated number of unique items after filtering\n",
    "    latent_factors = 10  # Example value\n",
    "    lr = 0.01  # Learning rate\n",
    "    reg = 0.01  # Regularization term\n",
    "\n",
    "    cross_validate_bpr(filtered_user_count, unique_item_count, latent_factors, lr, reg, filtered_train_df, filtered_test_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9987cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: theano_bpr in ./virtual/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: numpy in ./virtual/lib/python3.10/site-packages (from theano_bpr) (2.1.2)\n",
      "Requirement already satisfied: theano in ./virtual/lib/python3.10/site-packages (from theano_bpr) (1.0.5)\n",
      "Requirement already satisfied: scipy>=0.14 in ./virtual/lib/python3.10/site-packages (from theano->theano_bpr) (1.14.1)\n",
      "Requirement already satisfied: six>=1.9.0 in ./virtual/lib/python3.10/site-packages (from theano->theano_bpr) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install theano_bpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d7e74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9058c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bpr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano_bpr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_from_movielens\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano_bpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BPR\n\u001b[1;32m      4\u001b[0m bpr \u001b[38;5;241m=\u001b[39m BPR(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m181686\u001b[39m, \u001b[38;5;241m21570\u001b[39m)\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/theano_bpr/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# theano-bpr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2014 British Broadcasting Corporation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BPR\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [ BPR ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bpr'"
     ]
    }
   ],
   "source": [
    "from theano_bpr import BPR\n",
    "\n",
    "bpr = BPR(10, 181686, 21570)\n",
    "bpr.train(filtered_train_df, epochs=50)\n",
    "bpr.test(filtered_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21dca293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recommenders\n",
      "  Using cached recommenders-1.2.0-py3-none-any.whl (356 kB)\n",
      "Collecting transformers<5,>=4.27.0\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightfm<2,>=1.17\n",
      "  Using cached lightfm-1.17.tar.gz (316 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<2,>=1.2.0 in ./virtual/lib/python3.10/site-packages (from recommenders) (1.5.2)\n",
      "Collecting numba<1,>=0.57.0\n",
      "  Using cached numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: scipy>=1.10.1 in ./virtual/lib/python3.10/site-packages (from recommenders) (1.14.1)\n",
      "Collecting lightgbm<5,>=4.0.0\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0.0,>2.0.0 in ./virtual/lib/python3.10/site-packages (from recommenders) (2.2.3)\n",
      "Collecting pandera[strategies]>=0.15.0\n",
      "  Downloading pandera-0.20.4-py3-none-any.whl (259 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.6/259.6 KB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting category-encoders<3,>=2.6.0\n",
      "  Using cached category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
      "Collecting nltk<4,>=3.8.1\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting retrying<2,>=1.3.4\n",
      "  Using cached retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting memory-profiler<1,>=0.61.0\n",
      "  Using cached memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Collecting hyperopt<1,>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook<8,>=7.0.0 in ./virtual/lib/python3.10/site-packages (from recommenders) (7.2.2)\n",
      "Collecting scikit-surprise>=1.1.3\n",
      "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 KB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting seaborn<1,>=0.13.0\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting locust<3,>=2.12.2\n",
      "  Using cached locust-2.31.8-py3-none-any.whl (1.2 MB)\n",
      "Collecting cornac<2,>=1.15.2\n",
      "  Using cached cornac-1.18.0-cp310-cp310-manylinux1_x86_64.whl (21.3 MB)\n",
      "Collecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in ./virtual/lib/python3.10/site-packages (from category-encoders<3,>=2.6.0->recommenders) (2.1.2)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 KB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting powerlaw\n",
      "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 KB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in ./virtual/lib/python3.10/site-packages (from hyperopt<1,>=0.2.7->recommenders) (1.16.0)\n",
      "Requirement already satisfied: requests in ./virtual/lib/python3.10/site-packages (from lightfm<2,>=1.17->recommenders) (2.32.3)\n",
      "Collecting Werkzeug>=2.0.0\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 KB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ConfigArgParse>=1.5.5\n",
      "  Using cached ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
      "Collecting Flask-Cors>=3.0.10\n",
      "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting flask>=2.0.0\n",
      "  Using cached flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Collecting geventhttpclient>=2.3.1\n",
      "  Downloading geventhttpclient-2.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.6.0 in ./virtual/lib/python3.10/site-packages (from locust<3,>=2.12.2->recommenders) (4.12.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./virtual/lib/python3.10/site-packages (from locust<3,>=2.12.2->recommenders) (2.0.2)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in ./virtual/lib/python3.10/site-packages (from locust<3,>=2.12.2->recommenders) (26.2.0)\n",
      "Collecting gevent>=22.10.2\n",
      "  Downloading gevent-24.10.2-cp310-cp310-manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Flask-Login>=0.6.3\n",
      "  Using cached Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: psutil>=5.9.1 in ./virtual/lib/python3.10/site-packages (from locust<3,>=2.12.2->recommenders) (6.0.0)\n",
      "Collecting msgpack>=1.0.0\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in ./virtual/lib/python3.10/site-packages (from nltk<4,>=3.8.1->recommenders) (1.4.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in ./virtual/lib/python3.10/site-packages (from notebook<8,>=7.0.0->recommenders) (0.2.4)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./virtual/lib/python3.10/site-packages (from notebook<8,>=7.0.0->recommenders) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in ./virtual/lib/python3.10/site-packages (from notebook<8,>=7.0.0->recommenders) (2.27.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./virtual/lib/python3.10/site-packages (from notebook<8,>=7.0.0->recommenders) (6.4.1)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in ./virtual/lib/python3.10/site-packages (from notebook<8,>=7.0.0->recommenders) (4.2.5)\n",
      "Collecting numpy>=1.14.0\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./virtual/lib/python3.10/site-packages (from pandas<3.0.0,>2.0.0->recommenders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./virtual/lib/python3.10/site-packages (from pandas<3.0.0,>2.0.0->recommenders) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./virtual/lib/python3.10/site-packages (from pandas<3.0.0,>2.0.0->recommenders) (2024.2)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Collecting multimethod<=1.10.0\n",
      "  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n",
      "Collecting typing-inspect>=0.6.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting typeguard\n",
      "  Downloading typeguard-4.3.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./virtual/lib/python3.10/site-packages (from pandera[strategies]>=0.15.0->recommenders) (24.1)\n",
      "Collecting hypothesis>=6.92.7\n",
      "  Downloading hypothesis-6.115.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3.1.0 in ./virtual/lib/python3.10/site-packages (from scikit-learn<2,>=1.2.0->recommenders) (3.5.0)\n",
      "Collecting matplotlib!=3.6.1,>=3.4\n",
      "  Using cached matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./virtual/lib/python3.10/site-packages (from transformers<5,>=4.27.0->recommenders) (6.0.2)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./virtual/lib/python3.10/site-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (3.1.4)\n",
      "Collecting blinker>=1.6.2\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting zope.event\n",
      "  Using cached zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-7.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.3/254.3 KB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting greenlet>=3.1.1\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in ./virtual/lib/python3.10/site-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2024.8.30)\n",
      "Requirement already satisfied: urllib3 in ./virtual/lib/python3.10/site-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2.2.3)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.0 in ./virtual/lib/python3.10/site-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0->recommenders) (1.2.2)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./virtual/lib/python3.10/site-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0->recommenders) (24.2.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.21.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.5.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.7.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.8.3)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.14.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.10.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (8.6.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (23.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.18.1)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.10.0)\n",
      "Requirement already satisfied: overrides>=5.0 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (7.7.0)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (7.16.4)\n",
      "Requirement already satisfied: anyio>=3.1.0 in ./virtual/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.6.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (2.0.4)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (6.29.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (2.2.5)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (59.6.0)\n",
      "Requirement already satisfied: httpx>=0.25.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.27.2)\n",
      "Requirement already satisfied: babel>=2.10 in ./virtual/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (2.16.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./virtual/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (4.23.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./virtual/lib/python3.10/site-packages (from requests->lightfm<2,>=1.17->recommenders) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./virtual/lib/python3.10/site-packages (from requests->lightfm<2,>=1.17->recommenders) (3.4.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./virtual/lib/python3.10/site-packages (from Werkzeug>=2.0.0->locust<3,>=2.12.2->recommenders) (3.0.1)\n",
      "Collecting mpmath\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in ./virtual/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./virtual/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (21.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./virtual/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./virtual/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.14.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./virtual/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./virtual/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (1.8.7)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./virtual/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./virtual/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (1.6.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./virtual/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (8.28.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (0.20.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (2024.10.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./virtual/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.3.6)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./virtual/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.1.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./virtual/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in ./virtual/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.1.4)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (3.0.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.5.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.3.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.10.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.7.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (6.1.0)\n",
      "Requirement already satisfied: tinycss2 in ./virtual/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.3.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./virtual/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.20.0)\n",
      "Requirement already satisfied: ptyprocess in ./virtual/lib/python3.10/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.7.0)\n",
      "Requirement already satisfied: webencodings in ./virtual/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.5.1)\n",
      "Requirement already satisfied: decorator in ./virtual/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (5.1.1)\n",
      "Requirement already satisfied: stack-data in ./virtual/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./virtual/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (4.9.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./virtual/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (3.0.48)\n",
      "Requirement already satisfied: jedi>=0.16 in ./virtual/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.19.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (3.0.0)\n",
      "Requirement already satisfied: uri-template in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (24.8.0)\n",
      "Requirement already satisfied: isoduration in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (20.11.0)\n",
      "Requirement already satisfied: fqdn in ./virtual/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (1.5.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./virtual/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./virtual/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.6)\n",
      "Requirement already satisfied: pycparser in ./virtual/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./virtual/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in ./virtual/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.2.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./virtual/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (1.3.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./virtual/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in ./virtual/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./virtual/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (2.4.1)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./virtual/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (2.9.0.20241003)\n",
      "Using legacy 'setup.py install' for lightfm, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357259 sha256=17a484f8e6b83135e8c044a31c945da7530c4c79b621f5923fd3616a369d3480\n",
      "  Stored in directory: /home/iiitd/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: sortedcontainers, py4j, mpmath, brotli, zope.interface, zope.event, wrapt, Werkzeug, typeguard, tqdm, safetensors, retrying, regex, pyparsing, pydantic-core, pillow, numpy, networkx, mypy-extensions, multimethod, msgpack, memory-profiler, llvmlite, kiwisolver, itsdangerous, hypothesis, greenlet, future, fsspec, fonttools, filelock, cycler, ConfigArgParse, cloudpickle, click, blinker, annotated-types, typing-inspect, pydantic, patsy, numba, nltk, huggingface-hub, gevent, flask, contourpy, tokenizers, statsmodels, scikit-surprise, pandera, matplotlib, lightgbm, hyperopt, geventhttpclient, Flask-Login, Flask-Cors, transformers, seaborn, powerlaw, locust, lightfm, category-encoders, cornac, recommenders\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "  Running setup.py install for lightfm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed ConfigArgParse-1.7 Flask-Cors-5.0.0 Flask-Login-0.6.3 Werkzeug-3.0.4 annotated-types-0.7.0 blinker-1.8.2 brotli-1.1.0 category-encoders-2.6.4 click-8.1.7 cloudpickle-3.1.0 contourpy-1.3.0 cornac-1.18.0 cycler-0.12.1 filelock-3.16.1 flask-3.0.3 fonttools-4.54.1 fsspec-2024.9.0 future-1.0.0 gevent-24.10.2 geventhttpclient-2.3.1 greenlet-3.1.1 huggingface-hub-0.25.2 hyperopt-0.2.7 hypothesis-6.115.0 itsdangerous-2.2.0 kiwisolver-1.4.7 lightfm-1.17 lightgbm-4.5.0 llvmlite-0.43.0 locust-2.31.8 matplotlib-3.9.2 memory-profiler-0.61.0 mpmath-1.3.0 msgpack-1.1.0 multimethod-1.10 mypy-extensions-1.0.0 networkx-3.4.1 nltk-3.9.1 numba-0.60.0 numpy-2.0.2 pandera-0.20.4 patsy-0.5.6 pillow-10.4.0 powerlaw-1.5 py4j-0.10.9.7 pydantic-2.9.2 pydantic-core-2.23.4 pyparsing-3.2.0 recommenders-1.2.0 regex-2024.9.11 retrying-1.3.4 safetensors-0.4.5 scikit-surprise-1.1.4 seaborn-0.13.2 sortedcontainers-2.4.0 statsmodels-0.14.4 tokenizers-0.20.1 tqdm-4.66.5 transformers-4.45.2 typeguard-4.3.0 typing-inspect-0.9.0 wrapt-1.16.0 zope.event-5.0 zope.interface-7.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8f0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cornac in ./virtual/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: tqdm in ./virtual/lib/python3.10/site-packages (from cornac) (4.66.5)\n",
      "Requirement already satisfied: numpy in ./virtual/lib/python3.10/site-packages (from cornac) (2.0.2)\n",
      "Requirement already satisfied: scipy in ./virtual/lib/python3.10/site-packages (from cornac) (1.14.1)\n",
      "Requirement already satisfied: powerlaw in ./virtual/lib/python3.10/site-packages (from cornac) (1.5)\n",
      "Requirement already satisfied: matplotlib in ./virtual/lib/python3.10/site-packages (from powerlaw->cornac) (3.9.2)\n",
      "Requirement already satisfied: mpmath in ./virtual/lib/python3.10/site-packages (from powerlaw->cornac) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (24.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (3.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (1.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (4.54.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in ./virtual/lib/python3.10/site-packages (from matplotlib->powerlaw->cornac) (10.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./virtual/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->powerlaw->cornac) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./virtual/lib/python3.10/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cornac\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04854586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4279/1287199732.py\", line 3, in <module>\n",
      "    import cornac\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/__init__.py\", line 16, in <module>\n",
      "    from . import data\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/data/__init__.py\", line 18, in <module>\n",
      "    from .text import TextModality, ReviewModality\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/data/text.py\", line 27, in <module>\n",
      "    from ..utils import normalize\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/utils/__init__.py\", line 16, in <module>\n",
      "    from .common import validate_format\n",
      "  File \"/home/iiitd/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/utils/common.py\", line 21, in <module>\n",
      "    from .fast_sparse_funcs import (\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcornac\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecommenders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m movielens\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The Cornac Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_methods\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/data/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodality\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Modality\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodality\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureModality\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextModality, ReviewModality\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageModality\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphModality\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/data/text.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureModality\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodality\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fallback_feature\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m     29\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountVectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTextModality\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m PAD, UNK, BOS, EOS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<BOS>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/utils/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The Cornac Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_format\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimate_batches\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_rng\n",
      "File \u001b[0;32m~/bayesian_machine_learning/virtual/lib/python3.10/site-packages/cornac/utils/common.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfast_sparse_funcs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     inplace_csr_row_normalize_l1,\n\u001b[1;32m     23\u001b[0m     inplace_csr_row_normalize_l2\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m FLOAT_DTYPES \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n",
      "File \u001b[0;32mcornac/utils/fast_sparse_funcs.pyx:1\u001b[0m, in \u001b[0;36minit cornac.utils.fast_sparse_funcs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cornac\n",
    "import pandas as pd\n",
    "\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Model parameters\n",
    "NUM_FACTORS = 200\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "\n",
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "with Timer() as t:\n",
    "    bpr.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56daea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5828270,
     "sourceId": 9563586,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5828689,
     "sourceId": 9564152,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
