{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9563586,"sourceType":"datasetVersion","datasetId":5828270},{"sourceId":9564152,"sourceType":"datasetVersion","datasetId":5828689}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport pandas as pd","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>movieId</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>4.0</td>\n","      <td>944249077</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>1.0</td>\n","      <td>944250228</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>2.0</td>\n","      <td>943230976</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>30</td>\n","      <td>5.0</td>\n","      <td>944249077</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>5.0</td>\n","      <td>943228858</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userId  movieId  rating  timestamp\n","0       1       17     4.0  944249077\n","1       1       25     1.0  944250228\n","2       1       29     2.0  943230976\n","3       1       30     5.0  944249077\n","4       1       32     5.0  943228858"]},"metadata":{}}]},{"cell_type":"code","source":"df.drop( columns = ['rating' , 'timestamp'] , inplace = True)\n\ndf.head()","metadata":{},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>movieId</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>32</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userId  movieId\n","0       1       17\n","1       1       25\n","2       1       29\n","3       1       30\n","4       1       32"]},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>movieId</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>3.200020e+07</td>\n","      <td>3.200020e+07</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.002785e+05</td>\n","      <td>2.931861e+04</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>5.794905e+04</td>\n","      <td>5.095816e+04</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000e+00</td>\n","      <td>1.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>5.005300e+04</td>\n","      <td>1.233000e+03</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.002970e+05</td>\n","      <td>3.452000e+03</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.504510e+05</td>\n","      <td>4.419900e+04</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2.009480e+05</td>\n","      <td>2.927570e+05</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             userId       movieId\n","count  3.200020e+07  3.200020e+07\n","mean   1.002785e+05  2.931861e+04\n","std    5.794905e+04  5.095816e+04\n","min    1.000000e+00  1.000000e+00\n","25%    5.005300e+04  1.233000e+03\n","50%    1.002970e+05  3.452000e+03\n","75%    1.504510e+05  4.419900e+04\n","max    2.009480e+05  2.927570e+05"]},"metadata":{}}]},{"cell_type":"code","source":"%pip install scikit-learn","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting scikit-learn\n\n  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\nRequirement already satisfied: numpy>=1.19.5 in ./env/lib/python3.10/site-packages (from scikit-learn) (2.1.2)\n\nCollecting scipy>=1.6.0\n\n  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\nCollecting joblib>=1.2.0\n\n  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n\nCollecting threadpoolctl>=3.1.0\n\n  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n\nSuccessfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\n\nRangeIndex: 32000204 entries, 0 to 32000203\n\nData columns (total 2 columns):\n\n #   Column   Dtype\n\n---  ------   -----\n\n 0   userId   int64\n\n 1   movieId  int64\n\ndtypes: int64(2)\n\nmemory usage: 488.3 MB\n"}]},{"cell_type":"code","source":"#now splliting the data into training and testing split: \nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df.to_csv('train_data.txt' , index = False)\ntest_df.to_csv('test_data.txt', index=False)\n\n\ntrain_df.info()\ntest_df.info()","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport math\n\ndef topK_scores(test, predict, topk, user_count, item_count):\n    PrecisionSum = np.zeros(topk+1)\n    RecallSum = np.zeros(topk+1)\n    F1Sum = np.zeros(topk+1)\n    NDCGSum = np.zeros(topk+1)\n    OneCallSum = np.zeros(topk+1)\n    MRRSum = 0\n    MAPSum = 0\n    total_test_data_count = 0\n    \n    # Precompute DCGbest for efficiency\n    DCGbest = np.zeros(topk+1)\n    for k in range(1, topk+1):\n        DCGbest[k] = DCGbest[k - 1] + 1.0 / math.log2(k + 1)\n    \n    # Loop over each user\n    for i in range(user_count):\n        user_test = test[i * item_count:(i + 1) * item_count]\n        user_predict = predict[i * item_count:(i + 1) * item_count]\n\n        # Get test data size for this user\n        test_data_size = np.sum(user_test)\n        if test_data_size == 0:\n            continue\n        total_test_data_count += 1\n\n        # Get top-k item indices for this user\n        top_k_indices = np.argsort(user_predict)[-topk:][::-1]  # Sort in descending order\n        \n        hit_sum = 0\n        DCG = np.zeros(topk + 1)\n        for k in range(1, topk + 1):\n            item_id = top_k_indices[k - 1]\n            if user_test[item_id] == 1:\n                hit_sum += 1\n                DCG[k] = DCG[k - 1] + 1 / math.log2(k + 1)\n            else:\n                DCG[k] = DCG[k - 1]\n            \n            prec = hit_sum / k\n            rec = hit_sum / test_data_size\n            f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n\n            PrecisionSum[k] += prec\n            RecallSum[k] += rec\n            F1Sum[k] += f1\n            NDCGSum[k] += DCG[k] / DCGbest[k]\n            OneCallSum[k] += 1 if hit_sum > 0 else 0\n\n        # Compute MRR\n        for rank, idx in enumerate(top_k_indices, start=1):\n            if user_test[idx] == 1:\n                MRRSum += 1 / rank\n                break\n\n        # Compute MAP\n        AP = 0\n        hit_count = 0\n        for rank, idx in enumerate(top_k_indices, start=1):\n            if user_test[idx] == 1:\n                hit_count += 1\n                AP += hit_count / rank\n        MAPSum += AP / test_data_size\n    \n    # Normalize metrics\n    total_test_data_count = max(1, total_test_data_count)  # Avoid division by 0\n    print('MAP:', MAPSum / total_test_data_count)\n    print('MRR:', MRRSum / total_test_data_count)\n    print('Prec@5:', PrecisionSum[5] / total_test_data_count)\n    print('Rec@5:', RecallSum[5] / total_test_data_count)\n    print('F1@5:', F1Sum[5] / total_test_data_count)\n    print('NDCG@5:', NDCGSum[5] / total_test_data_count)\n    print('1-call@5:', OneCallSum[5] / total_test_data_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T03:39:32.551526Z","iopub.execute_input":"2024-10-07T03:39:32.552114Z","iopub.status.idle":"2024-10-07T03:39:32.576869Z","shell.execute_reply.started":"2024-10-07T03:39:32.552075Z","shell.execute_reply":"2024-10-07T03:39:32.575757Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom collections import defaultdict\n\n# Load the train and test data\ntrain_df = pd.read_csv(\"/kaggle/input/new-dataset/train_data.txt\", delimiter=\",\")\ntest_df = pd.read_csv(\"/kaggle/input/new-dataset/test_data.txt\", delimiter=\",\")\n\n# Convert the dataframes to lists of tuples\ntrain_user_item_pairs = list(train_df.itertuples(index=False, name=None))  # [(userid, movieid), ...]\ntest_user_item_pairs = list(test_df.itertuples(index=False, name=None))  # [(userid, movieid), ...]\n\n# Define the filtering function\ndef filter_data(user_item_pairs, min_interactions=10):\n    # Step 1: Count interactions\n    user_counts = defaultdict(set)\n    item_counts = defaultdict(set)\n    \n    for user, item in user_item_pairs:\n        user_counts[user].add(item)\n        item_counts[item].add(user)\n\n    # Step 2: Filter out users with less than `min_interactions` items\n    filtered_users = {user: items for user, items in user_counts.items() if len(items) >= min_interactions}\n    \n    # Step 3: Filter out items with less than `min_interactions` users\n    filtered_items = {item: users for item, users in item_counts.items() if len(users) >= min_interactions}\n\n    # Step 4: Remove any remaining users/items that no longer meet the conditions\n    while True:\n        # Remove items from users that don't exist in filtered items\n        new_filtered_users = {user: {item for item in items if item in filtered_items} for user, items in filtered_users.items()}\n        # Remove users that now have fewer than `min_interactions` items\n        new_filtered_users = {user: items for user, items in new_filtered_users.items() if len(items) >= min_interactions}\n        \n        # Remove users from items that don't exist in filtered users\n        new_filtered_items = {item: {user for user in users if user in new_filtered_users} for item, users in filtered_items.items()}\n        # Remove items that now have fewer than `min_interactions` users\n        new_filtered_items = {item: users for item, users in new_filtered_items.items() if len(users) >= min_interactions}\n\n        # Check if the filtering stabilized\n        if new_filtered_users == filtered_users and new_filtered_items == filtered_items:\n            break\n        \n        filtered_users, filtered_items = new_filtered_users, new_filtered_items\n\n    # Convert filtered data back to a list of duplets\n    filtered_user_item_pairs = [(user, item) for user, items in filtered_users.items() for item in items]\n    \n    return filtered_user_item_pairs\n\n# Apply filtering to train and test data\nfiltered_train_pairs = filter_data(train_user_item_pairs)\nfiltered_test_pairs = filter_data(test_user_item_pairs)\n\n# Convert the filtered pairs back to DataFrames\nfiltered_train_df = pd.DataFrame(filtered_train_pairs, columns=['userid', 'movieid'])\nfiltered_test_df = pd.DataFrame(filtered_test_pairs, columns=['userid', 'movieid'])\n\n# Overwrite the filtered DataFrames to their respective text files\nfiltered_train_df.to_csv(\"/kaggle/working/train_data_filtered.txt\", index=False, header=False)\nfiltered_test_df.to_csv(\"/kaggle/working/test_data_filtered.txt\", index=False, header=False)\n\nprint(\"Filtered train and test data saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:14:15.589452Z","iopub.execute_input":"2024-10-07T09:14:15.590352Z","iopub.status.idle":"2024-10-07T09:17:00.435801Z","shell.execute_reply.started":"2024-10-07T09:14:15.590306Z","shell.execute_reply":"2024-10-07T09:17:00.434539Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Filtered train and test data saved.\n","output_type":"stream"}]},{"cell_type":"code","source":"remaining_users = filtered_train_df['userid'].nunique()  # Count unique users\nremaining_items = filtered_train_df['movieid'].nunique()  # Count unique items\n\nprint(f\"Remaining users: {remaining_users}\")\nprint(f\"Remaining items: {remaining_items}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:17:00.437797Z","iopub.execute_input":"2024-10-07T09:17:00.438134Z","iopub.status.idle":"2024-10-07T09:17:00.623880Z","shell.execute_reply.started":"2024-10-07T09:17:00.438099Z","shell.execute_reply":"2024-10-07T09:17:00.622778Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Remaining users: 100646\nRemaining items: 21508\n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_train_df = filtered_train_df.sample(frac=0.001, random_state=42)\nfiltered_test_df=  filtered_test_df.sample(frac=0.001, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:17:00.625130Z","iopub.execute_input":"2024-10-07T09:17:00.625471Z","iopub.status.idle":"2024-10-07T09:17:01.465330Z","shell.execute_reply.started":"2024-10-07T09:17:00.625435Z","shell.execute_reply":"2024-10-07T09:17:01.464308Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import random\nfrom collections import defaultdict\n# import cupy as cp  # Import CuPy for GPU support\nfrom sklearn.metrics import roc_auc_score\n\nclass BPR:\n    def __init__(self, filtered_user_count, unique_item_count, latent_factors, lr, reg, filtered_train_df, filtered_test_df):\n        self.user_count = filtered_user_count\n        self.item_count = unique_item_count  # Unique item count\n        self.latent_factors = latent_factors\n        self.lr = lr\n        self.reg = reg\n        self.filtered_train_df = filtered_train_df  # Using filtered train DataFrame\n        self.filtered_test_df = filtered_test_df  # Using filtered test DataFrame\n\n        # Initialize latent factors for users and items\n        self.U = cp.random.rand(self.user_count, self.latent_factors).astype(cp.float32) * 0.01\n        self.V = cp.random.rand(self.item_count, self.latent_factors).astype(cp.float32) * 0.01\n        self.biasV = cp.random.rand(self.item_count).astype(cp.float32) * 0.01\n        \n        # Initialize test data using a dictionary for sparse representation\n        self.test_data = defaultdict(dict)  # Maps user index to item index with values\n        self.predict_ = cp.zeros(self.user_count * self.item_count, dtype=cp.float32)\n\n        # Initialize user and item mappings\n        self.user_mapping = {}\n        self.item_mapping = {}\n        self.reverse_user_mapping = {}\n        self.reverse_item_mapping = {}\n\n        # Create user and item mappings from filtered DataFrames\n        self.create_mappings()\n\n    def create_mappings(self):\n        user_id_set = self.filtered_train_df['userid'].unique()  # Extract unique user IDs\n        item_id_set = self.filtered_train_df['movieid'].unique()  # Extract unique item IDs\n\n        # Create a mapping for user IDs\n        for index, user_id in enumerate(user_id_set):\n            self.user_mapping[user_id] = index\n            self.reverse_user_mapping[index] = user_id\n\n        # Create a mapping for item IDs\n        for index, item_id in enumerate(item_id_set):\n            self.item_mapping[item_id] = index\n            self.reverse_item_mapping[index] = item_id\n\n    def load_test_data(self):\n        for _, row in self.filtered_test_df.iterrows():\n            user = int(row['userid'])\n            item = int(row['movieid'])\n\n            if user in self.user_mapping:  # Only process if user is in mapping\n                # Get the mapped index\n                mapped_user_index = self.user_mapping[user]\n                \n                if item in self.item_mapping:  # Check if the item is mapped\n                    mapped_item_index = self.item_mapping[item]\n                    # Use dictionary for test data\n                    self.test_data[mapped_user_index][mapped_item_index] = 1.0  # Mark interaction\n\n    def train(self, user_ratings_train, bootstrap=True):\n        \n            for _ in range(self.user_count):  # Number of iterations\n                try: \n                    # Bootstrap sample a user\n                    u_original = random.choice(list(user_ratings_train.keys())) if bootstrap else random.randint(1, self.user_count)\n                    if u_original not in user_ratings_train:\n                        continue\n\n                    # Map to the new user index\n                    u = self.user_mapping[u_original]\n\n                    # Sample a positive item (i) from user's interactions\n                    i = random.sample(user_ratings_train[u_original], 1)[0]  # Convert to list for sampling\n\n                    # Sample a negative item (j) not interacted by user\n                    j = random.randint(1, self.item_count)\n                    while j in user_ratings_train[u_original]:\n                        j = random.randint(1, self.item_count)\n\n                    # Decrement to make zero-indexed\n                    i = self.item_mapping[i]  # Map to new item index\n                    j = self.item_mapping[j]  # Map to new item index\n\n                    # Compute predictions for positive (i) and negative (j) items\n                    r_ui = cp.dot(self.U[u], self.V[i]) + self.biasV[i]\n                    r_uj = cp.dot(self.U[u], self.V[j]) + self.biasV[j]\n                    r_uij = r_ui - r_uj\n\n                    # BPR optimization: Compute the gradient and update using log-sigmoid\n                    loss_func = -1.0 / (1 + cp.exp(r_uij))  # Logistic function for BPR loss\n\n                    # Update latent factors for user and items\n                    self.U[u] += self.lr * (loss_func * (self.V[i] - self.V[j]) - self.reg * self.U[u])\n                    self.V[i] += self.lr * (loss_func * self.U[u] - self.reg * self.V[i])\n                    self.V[j] += self.lr * (-loss_func * self.U[u] - self.reg * self.V[j])\n\n                    # Update biases for items\n                    self.biasV[i] += self.lr * (loss_func - self.reg * self.biasV[i])\n                    self.biasV[j] += self.lr * (-loss_func - self.reg * self.biasV[j])\n                except:\n                    continue\n\n    def predict(self):\n        # Prediction matrix for user-item interactions\n        predict_matrix = cp.dot(self.U, self.V.T) + self.biasV\n        return predict_matrix\n\n    def evaluate(self):\n        # Evaluate using AUC score\n        predictions = self.predict()\n        # Flatten the test data using a comprehension since it's a dict\n        test_flat = cp.array([self.test_data[u].get(i, 0) for u in range(self.user_count) for i in range(self.item_count)], dtype=cp.float32)\n        auc_score = roc_auc_score(test_flat.get(), predictions.get())  # Convert CuPy arrays to NumPy arrays\n        print('AUC:', auc_score)\n\n    def main(self):\n        user_ratings_train = defaultdict(set)\n        \n        # Prepare user ratings from filtered_train_df\n        for _, row in self.filtered_train_df.iterrows():\n            user = int(row['userid'])\n            item = int(row['movieid'])\n            user_ratings_train[user].add(item)\n\n        self.load_test_data()\n\n        # Train model\n        for _ in range(1000):  # Number of training steps\n            self.train(user_ratings_train)\n        \n        # Evaluate model\n        self.evaluate()\n\n# Running the BPR model\nif __name__ == '__main__':\n    filtered_train_df = filtered_train_df.sample(frac=0.001, random_state=42)\n    filtered_test_df=  filtered_test_df.sample(frac=0.001, random_state=42)\n    filtered_user_count = 10064  # Number of unique users after filtering\n    latent_factors = 10  # Example value\n    lr = 0.01  # Learning rate\n    reg = 0.01  # Regularization term\n\n    # Initialize BPR model with filtered DataFrames\n    bpr = BPR(filtered_user_count, 2158, latent_factors, lr, reg, filtered_train_df, filtered_test_df)  # Using the correct unique item count\n    bpr.main()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:17:01.467582Z","iopub.execute_input":"2024-10-07T09:17:01.467985Z","iopub.status.idle":"2024-10-07T09:17:01.635250Z","shell.execute_reply.started":"2024-10-07T09:17:01.467945Z","shell.execute_reply":"2024-10-07T09:17:01.633763Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Regularization term\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Initialize BPR model with filtered DataFrames\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m bpr \u001b[38;5;241m=\u001b[39m \u001b[43mBPR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_user_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2158\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_test_df\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Using the correct unique item count\u001b[39;00m\n\u001b[1;32m    147\u001b[0m bpr\u001b[38;5;241m.\u001b[39mmain()\n","Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mBPR.__init__\u001b[0;34m(self, filtered_user_count, unique_item_count, latent_factors, lr, reg, filtered_train_df, filtered_test_df)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiltered_test_df \u001b[38;5;241m=\u001b[39m filtered_test_df  \u001b[38;5;66;03m# Using filtered test DataFrame\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize latent factors for users and items\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_count, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_factors)\u001b[38;5;241m.\u001b[39mastype(cp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_count, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_factors)\u001b[38;5;241m.\u001b[39mastype(cp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiasV \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_count)\u001b[38;5;241m.\u001b[39mastype(cp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'cp' is not defined"],"ename":"NameError","evalue":"name 'cp' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}}]}